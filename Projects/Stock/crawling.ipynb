{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# Anchor extraction from HTML document\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "with urlopen('https://www.naver.com/') as response:\n",
    "    soup = BeautifulSoup(response, 'html.parser')\n",
    "    for anchor in soup.select('span.ah_k'):\n",
    "        print(anchor)\n",
    "#urlopen 사용하면 매우 편하지만, 네이버 구글 등 막히는 추세임. 때문에 가상 웹브라우저를 사용해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://beomi.github.io/gb-crawling/posts/2017-09-28-HowToMakeWebCrawler-Headless-Chrome.html 에서 설명하는, 최종단계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "TEST_URL = 'https://www.naver.com'\n",
    "\n",
    "###############################################################for headless options\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "option.add_argument('window-size=1920x1080')\n",
    "option.add_argument(\"disable-gpu\")\n",
    "option.add_argument(\"user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\")\n",
    "option.add_argument(\"lang=ko_KR\") # 한국어 플러그인 추가\n",
    "options.add_argument(\"proxy-server=localhost:8080\") # Proxy server : mitmdump -p 8080 -s \"inject.ipynb\"\n",
    "driver=webdriver.Chrome('C:/Users/mod96/Desktop/HSH/Programs/WebCrawling/chromedriver', options=option)\n",
    "###############################################################\n",
    "\n",
    "driver.get(TEST_URL)\n",
    "\n",
    "###############################################################\n",
    "\n",
    "html = driver.page_source ## 페이지의 elements모두 가져오기\n",
    "soup = BeautifulSoup(html, 'html.parser') ## BeautifulSoup사용하기\n",
    "notices = soup.select('span.ah_k')\n",
    "\n",
    "print(notices)\n",
    "for n in notices:\n",
    "    print(n.text.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "proxy 안켜도 불러는 와지는듯?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 . 곽도원\n",
      "2 . 설국열차\n",
      "3 . 곽도원 나이\n",
      "4 . 김대명\n",
      "5 . 서대문구청\n",
      "6 . 김희원\n",
      "7 . 김대명 나이\n",
      "8 . n번방 사건\n",
      "9 . 국제수사\n",
      "10 . 김하영\n",
      "11 . 유민상 김하영\n",
      "12 . 도봉구청홈페이지\n",
      "13 . 환희\n",
      "14 . 오종혁\n",
      "15 . 유민상\n",
      "16 . 이상수\n",
      "17 . 이태원 클라쓰 16회 예고\n",
      "18 . 서대문구 확진자\n",
      "19 . 서대문구 확진자 동선\n",
      "20 . 전인화 나이\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "TEST_URL = 'https://datalab.naver.com/keyword/realtimeList.naver?where=main'\n",
    "\n",
    "############################################################### for headless options\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "option.add_argument('window-size=1920x1080')\n",
    "option.add_argument(\"disable-gpu\")\n",
    "driver=webdriver.Chrome('C:/Users/mod96/Desktop/HSH/Programs/WebCrawling/chromedriver', options=option)\n",
    "###############################################################\n",
    "\n",
    "driver.get(TEST_URL)\n",
    "\n",
    "###############################################################\n",
    "\n",
    "html = driver.page_source ## 페이지의 elements모두 가져오기\n",
    "soup = BeautifulSoup(html, 'html.parser') ## BeautifulSoup사용하기 <class 'bs4.BeautifulSoup'>\n",
    "hotKeys=soup.select(\"span.item_title\") ## 특정 부분만 가져오기 <class 'bs4.element.ResultSet'> <- select 안먹힘. select_one 써야함.\n",
    "for num,key in enumerate(hotKeys,1):\n",
    "    print(num,'.',key.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'> <class 'bs4.BeautifulSoup'>\n"
     ]
    }
   ],
   "source": [
    "print(type(hotKeys),\n",
    "type(soup))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주가 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'close': '161,200', 'high': '169,900', 'open': '163,700', 'low': '163,700'}\n",
      "{'close': '42,950', 'high': '45,500', 'open': '44,150', 'low': '43,550'}\n",
      "{'close': '140,000', 'high': '153,000', 'open': '145,500', 'low': '141,000'}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    "def get_bs_obj(company_code):\n",
    "    url = 'https://finance.naver.com/item/main.nhn?code=' + company_code\n",
    "    result = requests.get(url)\n",
    "    bs_obj = BeautifulSoup(result.content, 'html.parser')\n",
    "    return bs_obj\n",
    " \n",
    "def get_candle_chart(company_code):\n",
    "    bs_obj = get_bs_obj(company_code)\n",
    " \n",
    "    # close 종가(전일)\n",
    "    td_first = bs_obj.find('td', {'class': 'first'})  # 태그 td, 속성값 first 찾기\n",
    "    blind = td_first.find('span', {'class': 'blind'})  # 태그 span, 속성값 blind 찾기\n",
    "    close = blind.text\n",
    " \n",
    "    # high 고가\n",
    "    table = bs_obj.find('table', {'class': 'no_info'})  # 태그 table, 속성값 no_info 찾기\n",
    "    trs = table.find_all('tr')  # tr을 list로 []\n",
    "    first_tr = trs[0]  # 첫 번째 tr 지정\n",
    "    tds = first_tr.find_all('td')  # 첫 번째 tr 안에서 td를 list로\n",
    "    second_tds = tds[1]  # 두 번째 td 지정\n",
    "    high = second_tds.find('span', {'class': 'blind'}).text\n",
    " \n",
    "    # open 시가\n",
    "    second_tr = trs[1]  # 두 번째 tr 지정\n",
    "    tds_second_tr = second_tr.find_all('td')  # 두 번째 tr 안에서 td를 list로\n",
    "    first_td_in_second_tr = tds_second_tr[0]  # 첫 번째 td 지정\n",
    "    open = first_td_in_second_tr.find('span', {'class': 'blind'}).text\n",
    " \n",
    "    # low 저가\n",
    "    second_td_in_second_tr = tds_second_tr[1]  # 두 번째 td 지정\n",
    "    low = second_td_in_second_tr.find('span', {'class': 'blind'}).text\n",
    " \n",
    "    return {'close': close, 'high': high, 'open': open, 'low': low}\n",
    " \n",
    "# 펄어비스 회사 코드는 ”263750”\n",
    "# 삼성전자 회사 코드는 ”005930”\n",
    "# 셀트리온 회사 코드는 ”068270”\n",
    "company_codes = ['263750', '005930', '068270']\n",
    " \n",
    "for item in company_codes:\n",
    "    candle_chart = get_candle_chart(item)\n",
    "    print(candle_chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하루치 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5,500\n",
      "5,650\n",
      "5,590\n",
      "5,580\n",
      "5,600\n",
      "5,550\n",
      "5,500\n",
      "5,470\n",
      "5,420\n",
      "5,440\n",
      "5,390\n",
      "5,160\n",
      "5,290\n",
      "5,200\n",
      "5,240\n",
      "5,200\n",
      "5,240\n",
      "5,250\n",
      "5,250\n",
      "5,240\n",
      "5,260\n",
      "5,240\n",
      "5,200\n",
      "5,200\n",
      "5,240\n",
      "5,280\n",
      "5,250\n",
      "5,270\n",
      "5,260\n",
      "5,260\n",
      "5,250\n",
      "5,250\n",
      "5,220\n",
      "5,220\n",
      "5,220\n",
      "5,200\n",
      "5,170\n",
      "5,150\n",
      "5,150\n",
      "5,170\n",
      "5,180\n",
      "5,100\n",
      "5,090\n",
      "5,110\n",
      "5,120\n",
      "5,100\n",
      "5,110\n",
      "5,120\n",
      "5,130\n",
      "5,120\n",
      "5,110\n",
      "5,100\n",
      "5,060\n",
      "5,030\n",
      "5,020\n",
      "5,010\n",
      "5,070\n",
      "5,060\n",
      "5,040\n",
      "5,040\n",
      "5,030\n",
      "5,030\n",
      "5,100\n",
      "5,140\n",
      "5,120\n",
      "5,120\n",
      "5,110\n",
      "5,120\n",
      "5,130\n",
      "5,170\n",
      "5,170\n",
      "5,170\n",
      "5,160\n",
      "5,170\n",
      "5,150\n",
      "5,180\n",
      "5,200\n",
      "5,260\n",
      "5,250\n",
      "5,250\n",
      "5,250\n",
      "5,200\n",
      "5,230\n",
      "5,220\n",
      "5,220\n",
      "5,180\n",
      "5,200\n",
      "5,210\n",
      "5,230\n",
      "5,220\n",
      "5,240\n",
      "5,230\n",
      "5,230\n",
      "5,230\n",
      "5,220\n",
      "5,260\n",
      "5,270\n",
      "5,300\n",
      "5,340\n",
      "5,280\n",
      "5,280\n",
      "5,250\n",
      "5,260\n",
      "5,240\n",
      "5,240\n",
      "5,250\n",
      "5,260\n",
      "5,260\n",
      "5,280\n",
      "5,280\n",
      "5,300\n",
      "5,280\n",
      "5,320\n",
      "5,340\n",
      "5,360\n",
      "5,350\n",
      "5,380\n",
      "5,380\n",
      "5,410\n",
      "5,450\n",
      "5,550\n",
      "5,600\n",
      "5,520\n",
      "5,600\n",
      "5,590\n",
      "5,610\n",
      "5,580\n",
      "5,550\n",
      "5,600\n",
      "5,600\n",
      "5,600\n",
      "5,630\n",
      "5,640\n",
      "5,720\n",
      "5,760\n",
      "5,780\n",
      "5,840\n",
      "5,720\n",
      "5,780\n",
      "5,810\n",
      "5,810\n",
      "5,880\n",
      "5,900\n",
      "5,930\n",
      "5,920\n",
      "5,910\n",
      "5,830\n",
      "5,800\n",
      "5,830\n",
      "5,790\n",
      "5,750\n",
      "5,680\n",
      "5,720\n",
      "5,660\n",
      "5,640\n",
      "5,670\n",
      "5,680\n",
      "5,690\n",
      "5,700\n",
      "5,690\n",
      "5,670\n",
      "5,670\n",
      "5,670\n",
      "5,620\n",
      "5,660\n",
      "5,620\n",
      "5,660\n",
      "5,710\n",
      "5,750\n",
      "5,670\n",
      "5,720\n",
      "5,800\n",
      "5,770\n",
      "5,740\n",
      "5,730\n",
      "5,760\n",
      "5,740\n",
      "5,750\n",
      "5,650\n",
      "5,700\n",
      "5,640\n",
      "5,690\n",
      "5,660\n",
      "5,670\n",
      "5,650\n",
      "5,640\n",
      "5,600\n",
      "5,550\n",
      "5,580\n",
      "5,590\n",
      "5,580\n",
      "5,560\n",
      "5,540\n",
      "5,550\n",
      "5,570\n",
      "5,610\n",
      "5,600\n",
      "5,610\n",
      "5,610\n",
      "5,620\n",
      "5,670\n",
      "5,650\n",
      "5,660\n",
      "5,640\n",
      "5,630\n",
      "5,670\n",
      "5,670\n",
      "5,640\n",
      "5,680\n",
      "5,680\n",
      "5,710\n",
      "5,720\n",
      "5,900\n",
      "6,010\n",
      "6,000\n",
      "5,980\n",
      "5,890\n",
      "6,060\n",
      "6,080\n",
      "6,070\n",
      "6,020\n",
      "6,030\n",
      "6,000\n",
      "6,030\n",
      "5,960\n",
      "5,940\n",
      "5,900\n",
      "5,870\n",
      "5,880\n",
      "5,870\n",
      "5,870\n",
      "5,830\n",
      "5,820\n",
      "5,770\n",
      "5,780\n",
      "5,760\n",
      "5,780\n",
      "5,860\n",
      "5,840\n",
      "5,800\n",
      "5,820\n",
      "5,780\n",
      "5,790\n",
      "5,840\n",
      "5,800\n",
      "5,810\n",
      "5,810\n",
      "5,810\n",
      "5,810\n",
      "5,780\n",
      "5,820\n",
      "5,830\n",
      "5,860\n",
      "5,850\n",
      "5,830\n",
      "5,820\n",
      "5,810\n",
      "5,820\n",
      "5,830\n",
      "5,800\n",
      "5,740\n",
      "5,750\n",
      "5,800\n",
      "5,790\n",
      "5,770\n",
      "5,790\n",
      "5,790\n",
      "5,800\n",
      "5,830\n",
      "5,800\n",
      "5,780\n",
      "5,770\n",
      "5,800\n",
      "5,840\n",
      "5,830\n",
      "5,820\n",
      "5,820\n",
      "5,810\n",
      "5,800\n",
      "5,800\n",
      "5,780\n",
      "5,800\n",
      "5,800\n",
      "5,810\n",
      "5,810\n",
      "5,800\n",
      "5,800\n",
      "5,800\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,750\n",
      "5,770\n",
      "5,750\n",
      "5,750\n",
      "5,750\n",
      "5,770\n",
      "5,750\n",
      "5,760\n",
      "5,750\n",
      "5,740\n",
      "5,770\n",
      "5,760\n",
      "5,760\n",
      "5,740\n",
      "5,720\n",
      "5,720\n",
      "5,690\n",
      "5,660\n",
      "5,660\n",
      "5,650\n",
      "5,650\n",
      "5,610\n",
      "5,540\n",
      "5,580\n",
      "5,600\n",
      "5,570\n",
      "5,550\n",
      "5,600\n",
      "5,640\n",
      "5,640\n",
      "5,640\n",
      "5,680\n",
      "5,650\n",
      "5,650\n",
      "5,680\n",
      "5,660\n",
      "5,650\n",
      "5,700\n",
      "5,690\n",
      "5,670\n",
      "5,670\n",
      "5,670\n",
      "5,680\n",
      "5,680\n",
      "5,680\n",
      "5,680\n",
      "5,690\n",
      "5,680\n",
      "5,680\n",
      "5,680\n",
      "5,660\n",
      "5,660\n",
      "5,660\n",
      "5,650\n",
      "5,630\n",
      "5,630\n",
      "5,630\n",
      "5,610\n",
      "5,610\n",
      "5,630\n",
      "5,630\n",
      "5,640\n",
      "5,650\n",
      "5,650\n",
      "5,670\n",
      "5,650\n",
      "5,660\n",
      "5,670\n",
      "5,660\n",
      "5,690\n",
      "5,680\n",
      "5,670\n",
      "5,670\n",
      "5,680\n",
      "5,700\n",
      "5,700\n",
      "5,760\n",
      "5,780\n",
      "5,760\n",
      "5,760\n",
      "5,760\n",
      "5,740\n",
      "5,740\n",
      "5,730\n",
      "5,750\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n",
      "5,780\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "############################################################### for headless options\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "option.add_argument('window-size=1920x1080')\n",
    "option.add_argument(\"disable-gpu\")\n",
    "driver=webdriver.Chrome('C:/Users/mod96/Desktop/HSH/Programs/WebCrawling/chromedriver', options=option)\n",
    "###############################################################\n",
    "def Oneday(code,date): # 둘다 str 형식.\n",
    "    Price=[]\n",
    "    for i in range(39):\n",
    "        pagenum=str(i+1)\n",
    "        URL = 'https://finance.naver.com/item/sise_time.nhn?code='+code+'&thistime='+date+'&page='+pagenum\n",
    "        driver.get(URL)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ############################################\n",
    "        #time=soup.select(\"span.tah.p10.gray03\")\n",
    "        #T=[]\n",
    "        #for t in time:\n",
    "        #    T.append(t.text)\n",
    "        #print(T)\n",
    "        ############################################  \n",
    "        main=soup.select(\"td.num\")\n",
    "        M=[]\n",
    "        for m in main:\n",
    "            M.append(m.text)\n",
    "        k=0\n",
    "        while 6*k<len(M):\n",
    "            Price.append(M[6*k])\n",
    "            k=k+1\n",
    "        ############################################    \n",
    "    Price.reverse()\n",
    "    return Price\n",
    "            \n",
    "\n",
    "###############################################################\n",
    "Price=Oneday('272450','2020032016')\n",
    "for i in range(len(Price)):\n",
    "    print(Price[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oneday 로 일주일치까지 불러와지기 가능. KOSPI 는 'KOSPI' 임...\n",
    "\n",
    "<가장 간단한 러닝을 사용하기 위해 디자일을 해보자>\n",
    "\n",
    "input 은 관련된 모든 주의 하루 % + 코스피 % - https://finance.naver.com/item/sise_day.nhn?code=005930&page=1\n",
    "\n",
    "output 은 그중 하나의 다음날 %\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "% 받아오는 모듈을 만들어 보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.937\n",
      "-2.418\n",
      "-1.062\n",
      "-3.041\n",
      "1.476\n",
      "0.727\n",
      "3.61\n",
      "0.697\n",
      "-2.249\n",
      "-4.071\n",
      "0.738\n",
      "-4.579\n",
      "-2.495\n",
      "-1.673\n",
      "-2.102\n",
      "-3.272\n",
      "-3.594\n",
      "-5.811\n",
      "5.704\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "############################################################### for headless options\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "option.add_argument('window-size=1920x1080')\n",
    "option.add_argument(\"disable-gpu\")\n",
    "driver=webdriver.Chrome('C:/Users/mod96/Desktop/HSH/Programs/WebCrawling/chromedriver', options=option)\n",
    "###############################################################\n",
    "def Percentmodule(code,endpage): # 둘다 str 형식. 한 페이지에 10개.\n",
    "    PM=[];Price=[];Percent=[]\n",
    "    for i in range(int(endpage)):\n",
    "        pagenum=str(i+1)\n",
    "        URL = 'https://finance.naver.com/item/sise_day.nhn?code='+code+'&page='+pagenum\n",
    "        driver.get(URL)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ############################################ PM 은 전일비 가격 변동\n",
    "        main=soup.select(\"span.tah.p11.red02,span.tah.p11.nv01\")\n",
    "        for m in main:\n",
    "            if str(m).find('nv01')>0:\n",
    "                PM.append(-1*int(str(m.text).replace('\\t','').replace('\\n','').replace(',','')))\n",
    "            else:\n",
    "                PM.append(int(str(m.text).replace('\\t','').replace('\\n','').replace(',','')))\n",
    "        ############################################ Price 는 당일 종가\n",
    "        end=soup.select(\"td.num\")\n",
    "        M=[]\n",
    "        for e in end:\n",
    "            M.append(e.text)\n",
    "        k=0\n",
    "        while 6*k<len(M):\n",
    "            Price.append(int(str(M[6*k]).replace('\\t','').replace('\\n','').replace(',','')))\n",
    "            k=k+1\n",
    "        ############################################ 당일 PM/전날 Price = 증감율\n",
    "    for i in range(len(PM)-1):\n",
    "        Percent.append(round(100*PM[i]/Price[i+1],3))\n",
    "    Percent.reverse()\n",
    "    return Percent\n",
    "\n",
    "###############################################################\n",
    "Percent=Percentmodule('005930','2')\n",
    "for i in range(len(Percent)):\n",
    "    print(Percent[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "주요 지표들 url\n",
    "===========\n",
    "1. 환율 시세\n",
    "---------------페이지당 10개\n",
    "USD : 'https://finance.naver.com/marketindex/exchangeDailyQuote.nhn?marketindexCd=FX_USDKRW&page=1'\\\\\n",
    "EUR : 'https://finance.naver.com/marketindex/exchangeDailyQuote.nhn?marketindexCd=FX_EURKRW&page=1'\\\\\n",
    "JPY : 'https://finance.naver.com/marketindex/exchangeDailyQuote.nhn?marketindexCd=FX_JPYKRW&page=1'\\\\\n",
    "CNY : 'https://finance.naver.com/marketindex/exchangeDailyQuote.nhn?marketindexCd=FX_CNYKRW&page=1'\n",
    "\n",
    "IDX1=['USD','EUR','JPY','CNY']\\\\\n",
    "IDX2=['GSL','HGSL','LO']\\\\\n",
    "IDX3=['GC','SI','PA']\\\\\n",
    "IDX4=['020150','336370','098460','084850','011070','009150','028260','006400','018260','207940','005930','066570','000660','082270','039030','036830','005935','240810','000990']\n",
    "\n",
    "2. 자원 시세\n",
    "---------------페이지당 7개\n",
    "휘발유 : \n",
    "'https://finance.naver.com/marketindex/oilDailyQuote.nhn?marketindexCd=OIL_GSL&page=1'\\\\\n",
    "고급휘발유 : \n",
    "'https://finance.naver.com/marketindex/oilDailyQuote.nhn?marketindexCd=OIL_HGSL&page=1'\\\\\n",
    "경유 : \n",
    "'https://finance.naver.com/marketindex/oilDailyQuote.nhn?marketindexCd=OIL_LO&page=1'\\\\\n",
    "국제금 : \n",
    "'https://finance.naver.com/marketindex/worldDailyQuote.nhn?marketindexCd=CMDT_GC&fdtc=2&page=1'\\\\\n",
    "은 : \n",
    "'https://finance.naver.com/marketindex/worldDailyQuote.nhn?marketindexCd=CMDT_SI&fdtc=2&page=1'\\\\\n",
    "팔라듐 : \n",
    "'https://finance.naver.com/marketindex/worldDailyQuote.nhn?marketindexCd=CMDT_PA&fdtc=2&page=1'\\\\\n",
    "\n",
    "3. 주가\n",
    "전자기기 : ['020150','336370','098460','084850','011070','009150']\\\\\n",
    "삼성그룹 : ['028260','006400','018260','207940']\\\\\n",
    "전자,반도체(대기업) : ['005930','066570','000660']\\\\\n",
    "반도체(시총기준) : ['082270','039030','036830','005935','240810','000990']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합쳐봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "############################################################### for headless options\n",
    "option = webdriver.ChromeOptions()\n",
    "option.add_argument('headless')\n",
    "option.add_argument('window-size=1920x1080')\n",
    "option.add_argument(\"disable-gpu\")\n",
    "driver=webdriver.Chrome('C:/Users/mod96/Desktop/HSH/Programs/WebCrawling/chromedriver', options=option)\n",
    "#############################################################################################################################\n",
    "def Percentmodule1(code,endpage): # 둘다 str 형식. 한 페이지에 10개.\n",
    "    PM=[];Price=[];Percent=[]\n",
    "    for i in range(int(endpage)):\n",
    "        pagenum=str(i+1)\n",
    "        URL = 'https://finance.naver.com/marketindex/exchangeDailyQuote.nhn?marketindexCd=FX_'+code+'KRW&page='+pagenum\n",
    "        driver.get(URL)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ############################################ PM 은 전일비 가격 변동\n",
    "        main=soup.select(\"td.num\")\n",
    "        for m in main:\n",
    "            if str(m).find('https://ssl.pstatic.net/static/nfinance/ico_down.gif')>0:\n",
    "                PM.append(-1*float(str(m.text).replace('\\t','').replace('\\n','').replace(',','').replace(' ','')))\n",
    "            elif str(m).find('https://ssl.pstatic.net/static/nfinance/ico_up.gif')>0:\n",
    "                PM.append(float(str(m.text).replace('\\t','').replace('\\n','').replace(',','').replace(' ','')))\n",
    "            elif str(m).find('https://ssl.pstatic.net/static/nfinance/ico_same2.gif')>0:\n",
    "                PM.append(0)\n",
    "            else:\n",
    "                Price.append(float(str(m.text).replace('\\t','').replace('\\n','').replace(',','').replace(' ','')))\n",
    "    for i in range(len(PM)-1):\n",
    "        Percent.append(round(100*PM[i]/Price[i+1],3))\n",
    "    Percent.reverse()\n",
    "    return Percent\n",
    "#############################################################################################################################\n",
    "def Percentmodule2(code,endpage): # 둘다 str 형식. 한 페이지에 10개.\n",
    "    Percent=[]\n",
    "    for i in range(int(endpage)):\n",
    "        pagenum=str(i+1)\n",
    "        URL = 'https://finance.naver.com/marketindex/oilDailyQuote.nhn?marketindexCd=OIL_'+code+'&page='+pagenum\n",
    "        driver.get(URL)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ############################################ PM 은 전일비 가격 변동\n",
    "        main=soup.select(\"td.num\")\n",
    "        for m in main:\n",
    "            if str(m).find('%')>0:\n",
    "                Percent.append(float(str(m.text).replace('\\t','').replace('\\n','').replace(',','').replace('%','').replace(' ','')))\n",
    "    Percent.reverse()\n",
    "    del Percent[0]\n",
    "    return Percent\n",
    "#############################################################################################################################\n",
    "def Percentmodule3(code,endpage): # 둘다 str 형식. 한 페이지에 10개.\n",
    "    Percent=[]\n",
    "    for i in range(int(endpage)):\n",
    "        pagenum=str(i+1)\n",
    "        URL = 'https://finance.naver.com/marketindex/worldDailyQuote.nhn?marketindexCd=CMDT_'+code+'&fdtc=2&page='+pagenum\n",
    "        driver.get(URL)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ############################################ PM 은 전일비 가격 변동\n",
    "        main=soup.select(\"td.num\")\n",
    "        for m in main:\n",
    "            if str(m).find('%')>0:\n",
    "                Percent.append(float(str(m.text).replace('\\t','').replace('\\n','').replace(',','').replace('%','').replace(' ','')))\n",
    "    Percent.reverse()\n",
    "    del Percent[0]\n",
    "    return Percent\n",
    "#############################################################################################################################\n",
    "def Percentmodule4(code,endpage): # 둘다 str 형식. 한 페이지에 10개.\n",
    "    Price=[];Percent=[]\n",
    "    for i in range(int(endpage)):\n",
    "        pagenum=str(i+1)\n",
    "        URL = 'https://finance.naver.com/item/sise_day.nhn?code='+code+'&page='+pagenum\n",
    "        driver.get(URL)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ############################################ 당일 PM/전날 Price = 증감율\n",
    "        main=soup.select(\"td.num\")\n",
    "        M=[]\n",
    "        for e in main:\n",
    "            M.append(e.text)\n",
    "        k=0\n",
    "        while 6*k<len(M):\n",
    "            Price.append(float(str(M[6*k]).replace('\\t','').replace('\\n','').replace(',','').replace(' ','')))\n",
    "            k=k+1\n",
    "    for i in range(len(Price)-1):\n",
    "        Percent.append(round(100*(Price[i]-Price[i+1])/Price[i+1],3))\n",
    "    Percent.reverse()\n",
    "    return Percent\n",
    "###############################################################\n",
    "IDX1=['USD','EUR','JPY','CNY']  #환율\n",
    "IDX2=['GSL','HGSL','LO']  #유가\n",
    "IDX3=['GC','SI','PA']  #금은팔\n",
    "IDX4=['020150','098460','011070','009150','028260','006400','018260','207940','066570','000660','082270','039030','036830','005935','240810','000990','005930']\n",
    "l=len(IDX1)+len(IDX2)+len(IDX3)+len(IDX4)\n",
    "endpage1=77 #페이지당 10개\n",
    "endpage2=110 #페이지당 7개\n",
    "#820개 아래로 => 770개\n",
    "\n",
    "A=[]\n",
    "for code in IDX1:\n",
    "    A.append(Percentmodule1(code,endpage1))\n",
    "for code in IDX2:\n",
    "    A.append(Percentmodule2(code,endpage2))\n",
    "for code in IDX3:\n",
    "    A.append(Percentmodule3(code,endpage2))\n",
    "for code in IDX4:\n",
    "    A.append(Percentmodule4(code,endpage1))\n",
    "    \n",
    "import csv\n",
    "file=open(\"C:/Users/mod96/Desktop/HSH/data.csv\",'w',newline=\"\")\n",
    "csvwriter=csv.writer(file)\n",
    "for row in A:\n",
    "    csvwriter.writerow(row)\n",
    "file.close()\n",
    "\n",
    "import winsound as ws\n",
    "print(ws.Beep(2000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n",
      "769\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(A)):\n",
    "    print(len(A[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import winsound as ws\n",
    "print(ws.Beep(2000, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117.14285714285714"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "820/7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종페이지 확인하는 모듈\n",
    "=>[100, 100, 100, 100, 100, 100, 100, 83, 100, 100, 100, 100, 100, 100, 96, 100, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 100, 10, 100, 100, 100, 100, 100, 83, 100, 100, 100, 100, 100, 100, 96, 100, 100]\n"
     ]
    }
   ],
   "source": [
    "def whatpage(code,maxpage):\n",
    "    T=True;i=0;\n",
    "    while T==True and i<maxpage:\n",
    "        pagenum=str(i+1)\n",
    "        URL = 'https://finance.naver.com/item/sise_day.nhn?code='+code+'&page='+pagenum\n",
    "        driver.get(URL)\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        ############################################ PM 은 전일비 가격 변동\n",
    "        main=soup.select(\"td.num\")\n",
    "        M=[]\n",
    "        for e in main:\n",
    "            M.append(e.text)\n",
    "        k=0\n",
    "        while 6*k<len(M):\n",
    "            if str(M[6*k])=='\\xa0':\n",
    "                T=False;\n",
    "                break;\n",
    "            k=k+1\n",
    "        i=i+1\n",
    "    return i\n",
    "maxpage=100\n",
    "PAGE=[]\n",
    "IDX4=['020150','098460','084850','011070','009150','028260','006400','018260','207940','066570','000660','082270','039030','036830','005935','240810','000990','005930']\n",
    "for code in IDX4:\n",
    "    PAGE.append(whatpage(code,maxpage))    \n",
    "print(PAGE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "csv 저장부터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "A=[[1,2,3],[4,5,6],[7,8,9],[10,11,12]]\n",
    "import csv\n",
    "\n",
    "file=open(\"C:/Users/mod96/Desktop/HSH/data.csv\",'w',newline=\"\")\n",
    "csvwriter=csv.writer(file)\n",
    "for row in A:\n",
    "    csvwriter.writerow(row)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "IDX1=['USD','EUR','JPY','CNY']  #환율\n",
    "IDX2=['GSL','HGSL','LO']  #유가\n",
    "IDX3=['GC','SI','PA']  #금은팔\n",
    "IDX4=['020150','336370','098460','084850','011070','009150','028260','006400','018260','207940','066570','000660','082270','039030','036830','005935','240810','000990','005930']\n",
    "\n",
    "l=len(IDX1)+len(IDX2)+len(IDX3)+len(IDX4)\n",
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(16, input_dim=l-1, activation='relu'),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')    \n",
    "    ])\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected dense_input to have shape (26,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-28b53e5be262>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    594\u001b[0m     \u001b[0mval_adapter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, shuffle, standardize_function, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mstandardize_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    656\u001b[0m       x, y, sample_weights = standardize_function(\n\u001b[1;32m--> 657\u001b[1;33m           x=x, y=y, sample_weight=sample_weights)\n\u001b[0m\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    659\u001b[0m     self._internal_adapter = TensorLikeDataAdapter(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    580\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected dense_input to have shape (26,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "X=A[:len(A)-1][:len(A[0])-1]\n",
    "Y=A[len(A)-1][1:]\n",
    "model.fit(X, Y, epochs=10)\n",
    "model.evaluate(X,  Y, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version 03.25. - 크롤링 완료. 러닝을 고차원화 해야함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
